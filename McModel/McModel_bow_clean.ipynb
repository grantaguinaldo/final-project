{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.calibration import calibration_curve\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://bit.ly/mc_clean_data_usc'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_df = 0.01\n",
    "max_df = 0.95\n",
    "vect = CountVectorizer(stop_words='english', min_df=min_df, max_df=max_df, ngram_range=(1, 2))\n",
    "\n",
    "\n",
    "X = df['trim_text']\n",
    "y = df['rude']\n",
    "\n",
    "X_dtm = vect.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "type(X_dtm_1)\n",
    "review_len = np.array(df['review_len']).astype(float)\n",
    "sentiment_polarity = np.array(df['sentiment_polarity']).astype(float)\n",
    "sentiment_objectivity = np.array(df['sentiment_objectivity']).astype(float)\n",
    "\n",
    "X_dtm_2 = hstack((X_dtm_1, review_len[:,None])).tocsr() \n",
    "X_dtm_3 = hstack((X_dtm_2, sentiment_polarity[:,None])).tocsr() \n",
    "X_dtm = hstack((X_dtm_3, sentiment_objectivity[:,None])).tocsr() \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_dtm, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive_Bayes (`MultinomialNB`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "\n",
    "# Create the GridSearch estimator along with a parameter object containing the values to adjust\n",
    "param_grid = {'alpha': [0, 1],\n",
    "              'fit_prior': ['True','False']}\n",
    "\n",
    "grid = GridSearchCV(nb, param_grid, verbose=3, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Fit the model using the grid search estimator. \n",
    "# This will take the Naive Bayes model and try each combination of parameters\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    " # List the best score\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the best parameters for this dataset\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Make predictions with the hypertuned model\n",
    "predictions = grid.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predictions,\n",
    "                            target_names=[\"not rude\", \"rude\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB(alpha=1.0e-10, fit_prior=True)\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_class = nb.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_roc_auc = roc_auc_score(y_test, y_pred_class)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_class)\n",
    "\n",
    "plt.figure()\n",
    "plt.grid()\n",
    "\n",
    "plt.plot(fpr, tpr, label='Naive Bayes (area = %0.2f)' % nb_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(nb, X_train, y_train, cv=10)\n",
    "\n",
    "plt.title(\"Learning Curves (Naive Bayes)\")\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.fill_between(train_sizes, \n",
    "                 train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, \n",
    "                 alpha=0.1,\n",
    "                 color=\"r\")\n",
    "\n",
    "plt.fill_between(train_sizes, \n",
    "                 test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, \n",
    "                 alpha=0.1, color=\"g\")\n",
    "\n",
    "plt.plot(train_sizes, \n",
    "         train_scores_mean, \n",
    "         'o-', \n",
    "         color=\"r\", \n",
    "         label=\"Training score\")\n",
    "\n",
    "plt.plot(train_sizes, \n",
    "         test_scores_mean, \n",
    "         'o-', color=\"g\", \n",
    "         label=\"Cross-validation score\")\n",
    "\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive_Bayes (`GaussianNB`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train.toarray(), y_train)\n",
    "y_pred_class = nb.predict(X_test.toarray())\n",
    "confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_roc_auc = roc_auc_score(y_test, y_pred_class)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_class)\n",
    "\n",
    "plt.figure()\n",
    "plt.grid()\n",
    "\n",
    "plt.plot(fpr, tpr, label='Gaussian Naive Bayes (area = %0.2f)' % gnb_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(gnb, X_train.toarray(), y_train, cv=10)\n",
    "\n",
    "plt.title(\"Learning Curves (Gaussian Naive Bayes)\")\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.fill_between(train_sizes, \n",
    "                 train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, \n",
    "                 alpha=0.1,\n",
    "                 color=\"r\")\n",
    "\n",
    "plt.fill_between(train_sizes, \n",
    "                 test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, \n",
    "                 alpha=0.1, color=\"g\")\n",
    "\n",
    "plt.plot(train_sizes, \n",
    "         train_scores_mean, \n",
    "         'o-', \n",
    "         color=\"r\", \n",
    "         label=\"Training score\")\n",
    "\n",
    "plt.plot(train_sizes, \n",
    "         test_scores_mean, \n",
    "         'o-', color=\"g\", \n",
    "         label=\"Cross-validation score\")\n",
    "\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lg = LogisticRegression()\n",
    "\n",
    "# Create regularization penalty space\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# Create regularization hyperparameter space\n",
    "C = np.logspace(0, 4, 10)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "grid = GridSearchCV(lg, hyperparameters, verbose=3, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Fit the model using the grid search estimator. \n",
    "# This will take the logistic regression  model and try each combination of parameters\n",
    "\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the best parameters for this dataset\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the best score\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Make predictions with the hypertuned model\n",
    "predictions = grid.predict(X_test)\n",
    "\n",
    " # Calculate classification report\n",
    "print(classification_report(y_test, predictions,\n",
    "                            target_names=[\"not rude\", \"rude\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression(penalty='l1', C=2.7825594022071245)\n",
    "\n",
    "lg.fit(X_train, y_train)\n",
    "y_pred_class = lg.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_roc_auc = roc_auc_score(y_test, y_pred_class)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_class)\n",
    "\n",
    "plt.figure()\n",
    "plt.grid()\n",
    "\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % lg_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(lg, X_train, y_train, cv=10)\n",
    "\n",
    "plt.title(\"Learning Curves (Logistic Regression)\")\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.fill_between(train_sizes, \n",
    "                 train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, \n",
    "                 alpha=0.1,\n",
    "                 color=\"r\")\n",
    "\n",
    "plt.fill_between(train_sizes, \n",
    "                 test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, \n",
    "                 alpha=0.1, color=\"g\")\n",
    "\n",
    "plt.plot(train_sizes, \n",
    "         train_scores_mean, \n",
    "         'o-', \n",
    "         color=\"r\", \n",
    "         label=\"Training score\")\n",
    "\n",
    "plt.plot(train_sizes, \n",
    "         test_scores_mean, \n",
    "         'o-', color=\"g\", \n",
    "         label=\"Cross-validation score\")\n",
    "\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loop through different k values to see which has the highest accuracy\n",
    "# Note: We only use odd numbers because we don't want any ties\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for k in range(1, 40, 2):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_score = knn.score(X_train, y_train)\n",
    "    test_score = knn.score(X_test, y_test)\n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "train_sizes = [num for num in range(1, 40, 2)]\n",
    "\n",
    "plt.title(\"k-Nearest Neighbors Complexity Curve\")\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.plot(train_sizes, \n",
    "         train_scores, \n",
    "         'o-', \n",
    "         color=\"r\", \n",
    "         label=\"Training score\")\n",
    "\n",
    "plt.plot(train_sizes, \n",
    "         test_scores, \n",
    "         'o-', color=\"g\", \n",
    "         label=\"Cross-validation score\")\n",
    "\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=11)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Make predictions with the hypertuned model\n",
    "y_pred_class = knn.predict(X_test)\n",
    "\n",
    " # Calculate classification report\n",
    "print(classification_report(y_test, y_pred_class,\n",
    "                            target_names=[\"not rude\", \"rude\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_roc_auc = roc_auc_score(y_test, y_pred_class)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_class)\n",
    "\n",
    "plt.figure()\n",
    "plt.grid()\n",
    "\n",
    "plt.plot(fpr, tpr, label='k-Nearest Neighbors (area = %0.2f)' % knn_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(knn, X_train, y_train, cv=10)\n",
    "\n",
    "plt.title(\"k-Nearest Neighbors Learning Curve\")\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.fill_between(train_sizes, \n",
    "                 train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, \n",
    "                 alpha=0.1,\n",
    "                 color=\"r\")\n",
    "\n",
    "plt.fill_between(train_sizes, \n",
    "                 test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, \n",
    "                 alpha=0.1, color=\"g\")\n",
    "\n",
    "plt.plot(train_sizes, \n",
    "         train_scores_mean, \n",
    "         'o-', \n",
    "         color=\"r\", \n",
    "         label=\"Training score\")\n",
    "\n",
    "plt.plot(train_sizes, \n",
    "         test_scores_mean, \n",
    "         'o-', color=\"g\", \n",
    "         label=\"Cross-validation score\")\n",
    "\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the SVC Model\n",
    "from sklearn.svm import SVC \n",
    "model = SVC()\n",
    "\n",
    "# Create the GridSearch estimator along with a parameter object containing the values to adjust\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [1, 5, 10, 50],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005],\n",
    "              'kernel': ['linear', 'poly', 'rbf']}\n",
    "grid = GridSearchCV(model, param_grid, verbose=3, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Fit the model using the grid search estimator. \n",
    "# This will take the SVC model and try each combination of parameters\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # List the best score\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List the best parameters for this dataset\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Make predictions with the hypertuned model\n",
    "predictions = grid.predict(X_test)\n",
    "\n",
    " # Calculate classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions,\n",
    "                            target_names=[\"not rude\", \"rude\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SVC(kernel='rbf', C=50, gamma=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = model.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_roc_auc = roc_auc_score(y_test, y_pred_class)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_class)\n",
    "\n",
    "plt.figure()\n",
    "plt.grid()\n",
    "\n",
    "plt.plot(fpr, tpr, label='k-Nearest Neighbors (area = %0.2f)' % svm_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(model, X_train, y_train, cv=10)\n",
    "\n",
    "plt.title(\"SVM (Linear) Learning Curve\")\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.fill_between(train_sizes, \n",
    "                 train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, \n",
    "                 alpha=0.1,\n",
    "                 color=\"r\")\n",
    "\n",
    "plt.fill_between(train_sizes, \n",
    "                 test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, \n",
    "                 alpha=0.1, color=\"g\")\n",
    "\n",
    "plt.plot(train_sizes, \n",
    "         train_scores_mean, \n",
    "         'o-', \n",
    "         color=\"r\", \n",
    "         label=\"Training score\")\n",
    "\n",
    "plt.plot(train_sizes, \n",
    "         test_scores_mean, \n",
    "         'o-', color=\"g\", \n",
    "         label=\"Cross-validation score\")\n",
    "\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "gnb = GaussianNB()\n",
    "lg = LogisticRegression(penalty='l1', C=2.7825594022071245)\n",
    "knn = KNeighborsClassifier(n_neighbors=11)\n",
    "model = SVC(kernel='rbf', C=50, gamma=0.001)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "for clf, name in [(nb, 'Multinomial NB'),\n",
    "                  (gnb, 'GaussianNB'),\n",
    "                  (lg, 'Logistic Regression'),\n",
    "                  (knn, 'Random Forest'),\n",
    "                  (model, 'Support Vector Machine')]:\n",
    "    clf.fit(X_train.toarray(), y_train)\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        prob_pos = clf.predict_proba(X_test.toarray())[:, 1]\n",
    "    else:  # use decision function\n",
    "        prob_pos = clf.decision_function(X_test.toarray())\n",
    "        prob_pos = (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(y_test, prob_pos, n_bins=10)\n",
    "\n",
    "    ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",\n",
    "             label=\"%s\" % (name, ))\n",
    "\n",
    "    ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n",
    "             histtype=\"step\", lw=2)\n",
    "\n",
    "ax1.set_ylabel(\"Fraction of positives\")\n",
    "ax1.set_ylim([-0.05, 1.05])\n",
    "ax1.legend(loc=\"lower right\")\n",
    "ax1.set_title('Calibration plots  (reliability curve)')\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "ax2.set_xlabel(\"Mean predicted value\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.legend(loc=\"upper center\", ncol=2)\n",
    "\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
